{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0054aae",
   "metadata": {},
   "source": [
    "# Here is some approaches that will be used while matchmaking\n",
    "\n",
    "- Some of them better for large amount of users comparing in performance (Clustering + SA)\n",
    "- Some will be better for average amount of users (Clustering + Greedy, GA)\n",
    "- Other better for small (greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7df91b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rail/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/rail/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, DefaultDict\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from math import exp\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d50146",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ffa3c",
   "metadata": {},
   "source": [
    "#### Users_data (users that will be used in matching)\n",
    "```json\n",
    "{\n",
    "    \"users_data\": [\n",
    "        {\n",
    "            \"user_id\": \"...\",\n",
    "            \"bio\": \"...\",\n",
    "            \"tags\": [\"...\", \"...\"]\n",
    "        }\n",
    "        // more users\n",
    "    ]\n",
    "}\n",
    "```\n",
    "#### History\n",
    "```json\n",
    "{\n",
    "    \"history\": [\n",
    "        {//one session\n",
    "            \"user_id1\": \"...\",\n",
    "            \"user_id2\": \"...\",\n",
    "            \"timestamp\": \"...\"\n",
    "        }\n",
    "        // other\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1282d46",
   "metadata": {},
   "source": [
    "## Get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1415c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio(df):\n",
    "   pass \n",
    "\n",
    "def get_history(df):\n",
    "   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5c82e",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edac5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text: str, stem_words: bool = True, remove_stopwords: bool = True) -> str:\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    if stem_words:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a86622",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac94a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = [\n",
    "    {\n",
    "        \"user_id\": str(i),\n",
    "        \"bio\": random.choice([\n",
    "            f\"Software engineer passionate about {random.choice(['AI', 'web development', 'cloud computing'])}\",\n",
    "            f\"Data scientist specializing in {random.choice(['NLP', 'computer vision', 'time series'])}\",\n",
    "            f\"UX designer with focus on {random.choice(['mobile apps', 'accessibility', 'user research'])}\",\n",
    "            f\"Product manager interested in {random.choice(['blockchain', 'edtech', 'healthtech'])}\",\n",
    "            \"Digital marketer growth hacking enthusiast\",\n",
    "            \"DevOps engineer building scalable infrastructure\",\n",
    "            \"Cybersecurity expert focusing on penetration testing\"\n",
    "        ]),\n",
    "        \"tags\": random.sample([\n",
    "            \"tech\", \"AI\", \"programming\", \"data\", \"design\", \n",
    "            \"startups\", \"entrepreneurship\", \"marketing\",\n",
    "            \"crypto\", \"health\", \"education\", \"finance\",\n",
    "            \"music\", \"art\", \"travel\", \"sports\", \"reading\"\n",
    "        ], k=random.randint(2, 5))\n",
    "    } \n",
    "    for i in range(1, 32)\n",
    "]\n",
    "\n",
    "\n",
    "history = []\n",
    "user_ids = [str(i) for i in range(1, 32)]\n",
    "date = datetime.now() - timedelta(days=180) \n",
    "\n",
    "for _ in range(20):\n",
    "    user1, user2 = random.sample(user_ids, 2)\n",
    "    history.append({\n",
    "        \"user_id1\": user1,\n",
    "        \"user_id2\": user2,\n",
    "        \"timestamp\": date.strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "    date += timedelta(days=random.randint(7, 21))\n",
    "    \n",
    "    user_ids.remove(user1)\n",
    "    if user2 in user_ids:\n",
    "        user_ids.remove(user2)\n",
    "    if len(user_ids) < 2:\n",
    "        user_ids = [str(i) for i in range(1, 32)] \n",
    "\n",
    "for user in users_data:\n",
    "    user[\"bio\"] = preprocess_text(user[\"bio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efc7d4",
   "metadata": {},
   "source": [
    "## Tag distances\n",
    "Must return value such that: \n",
    "- 0 are more similar, ..., 1 are not similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcef8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(tags1: List[str], tags2: List[str]) -> float:\n",
    "    set1, set2 = set(tags1), set(tags2)\n",
    "    \n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "\n",
    "    return 1 - intersection / union if union != 0 else 1.0\n",
    "\n",
    "def semantic_tag_distance(tags1: List[str], tags2: List[str], embedder: SentenceTransformer) -> float:\n",
    "    text1 = \" \".join(tags1)\n",
    "    text2 = \" \".join(tags2)\n",
    "\n",
    "    emb1 = embedder.encode(text1)\n",
    "    emb2 = embedder.encode(text2)\n",
    "    if hasattr(emb1, \"cpu\"):\n",
    "        emb1 = emb1.cpu().numpy()\n",
    "    if hasattr(emb2, \"cpu\"):\n",
    "        emb2 = emb2.cpu().numpy()\n",
    "    \n",
    "    return 1 - cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757f0e7",
   "metadata": {},
   "source": [
    "## Genetics approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b6210df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tag_distance_func: str = \"jaccard\",  # \"jaccard\" or \"semantic\"\n",
    "        coef_a: float = 0.6,\n",
    "        coef_b: float = 0.4,\n",
    "        penalty_multiplier: float = 0.9,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        pop_size: int = 50,\n",
    "        generations: int = 100,\n",
    "        mutation_rate: float = 0.1\n",
    "    ):\n",
    "        self.coef_a = coef_a\n",
    "        self.coef_b = coef_b\n",
    "        self.penalty = penalty_multiplier\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.tag_distance = {\n",
    "            \"jaccard\": jaccard_distance,\n",
    "            \"semantic\": lambda t1, t2: semantic_tag_distance(t1, t2, self.embedder)\n",
    "        }.get(tag_distance_func, jaccard_distance)\n",
    "        \n",
    "\n",
    "    def compute_pair_score(self, user1: Dict, user2: Dict, history: List[Dict]) -> float:\n",
    "        bio_sim = cosine_similarity([user1[\"bio_embedding\"]], [user2[\"bio_embedding\"]])[0][0]\n",
    "        \n",
    "        tag_dist = self.tag_distance(user1[\"tags\"], user2[\"tags\"])\n",
    "        \n",
    "        base_score = self.coef_a * bio_sim + self.coef_b * (1 - tag_dist)\n",
    "        \n",
    "        weeks_passed = self.get_weeks_since_last_meeting(user1[\"user_id\"], user2[\"user_id\"],history)\n",
    "        penalty = self.penalty ** weeks_passed if weeks_passed != float(\"inf\") else 0\n",
    "\n",
    "        return base_score * (1 - penalty)\n",
    "    \n",
    "    def get_weeks_since_last_meeting(self, user_id1: str, user_id2: str, history: List[Dict]) -> float:\n",
    "        for event in history:\n",
    "            if {event[\"user_id1\"], event[\"user_id2\"]} == {user_id1, user_id2}:\n",
    "                last_meeting = datetime.strptime(event[\"timestamp\"], \"%Y-%m-%d\")\n",
    "                return (datetime.now() - last_meeting).days // 7\n",
    "            \n",
    "        return float(\"inf\")\n",
    "    \n",
    "    def create_individual(self, users: List[Dict]) -> List[Tuple[int, int]]:\n",
    "        indices = list(range(len(users)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        pairs = [(indices[i], indices[i+1]) for i in range(0, len(indices)-1, 2)]\n",
    "        \n",
    "        if len(indices) % 2 != 0:\n",
    "            pairs.append((indices[-1], -1))\n",
    "            \n",
    "        return pairs\n",
    "    \n",
    "    def fitness(self, individual: List[Tuple[int, int]], users: List[Dict], history: List[Dict]) -> float:\n",
    "        total_score = 0.0\n",
    "        valid_pairs = 0\n",
    "        count_non_valid = 0\n",
    "        for i, j in individual:\n",
    "            if j == -1:\n",
    "                if count_non_valid == 0:\n",
    "                    count_non_valid += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    score -= 1\n",
    "                    valid_pairs += 1\n",
    "                    continue\n",
    "            score = self.compute_pair_score(users[i], users[j], history)\n",
    "            total_score += score\n",
    "            valid_pairs += 1\n",
    "\n",
    "        return total_score / valid_pairs if valid_pairs > 0 else 0.0\n",
    "    \n",
    "    def crossover(self, parent1: List[Tuple[int, int]], parent2: List[Tuple[int, int]], users: List[Dict]) -> List[Tuple[int, int]]:\n",
    "        crossover_point = random.randint(1, len(parent1)-1)\n",
    "        child = parent1[:crossover_point] + parent2[crossover_point:]\n",
    "\n",
    "        return self.repair_individual(child, users)\n",
    "    \n",
    "    def mutate(self, individual: List[Tuple[int, int]], users: List[Dict]) -> List[Tuple[int, int]]:\n",
    "\n",
    "        if random.random() < self.mutation_rate:\n",
    "            idx1, idx2 = random.sample(range(len(individual)), 2)\n",
    "            i1, j1 = individual[idx1]\n",
    "            i2, j2 = individual[idx2]\n",
    "            individual[idx1] = (i1, j2)\n",
    "            individual[idx2] = (i2, j1)\n",
    "\n",
    "        return self.repair_individual(individual, users)\n",
    "    \n",
    "    \n",
    "    def repair_individual(self, individual: List[Tuple[int, int]], users: List[Dict]) -> List[Tuple[int, int]]:\n",
    "        used = set()\n",
    "        new_individual = []\n",
    "        unpaired = None\n",
    "        \n",
    "        for i, j in individual:\n",
    "            if i not in used and (j == -1 or j not in used):\n",
    "                if j == -1:\n",
    "                    if unpaired is None:\n",
    "                        unpaired = i\n",
    "                        used.add(i)\n",
    "                        new_individual.append((i, -1))\n",
    "                else:\n",
    "                    new_individual.append((i, j))\n",
    "                    used.add(i)\n",
    "                    used.add(j)\n",
    "        \n",
    "        all_users = set(range(len(users)))\n",
    "        leftover = list(all_users - used)\n",
    "        \n",
    "        for k in range(0, len(leftover)-1, 2):\n",
    "            new_individual.append((leftover[k], leftover[k+1]))\n",
    "        \n",
    "        if len(leftover) % 2 != 0:\n",
    "            if unpaired is None:\n",
    "                new_individual.append((leftover[-1], -1))\n",
    "        \n",
    "        return new_individual\n",
    "\n",
    "    \n",
    "\n",
    "    def match(self, users_data: List[Dict], history: List[Dict] = None) -> List[Tuple[str, Optional[str]]]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "        \n",
    "        for user in users_data:\n",
    "            user[\"bio_embedding\"] = self.embedder.encode(user[\"bio\"])\n",
    "            if hasattr(user[\"bio_embedding\"], \"cpu\"):\n",
    "                user[\"bio_embedding\"] = user[\"bio_embedding\"].cpu().numpy()\n",
    "        \n",
    "        population = [self.create_individual(users_data) for _ in range(self.pop_size)]\n",
    "        \n",
    "        for generation in range(self.generations):\n",
    "            print(f\"Generation {generation+1}/{self.generations}\")\n",
    "            fitness_scores = [self.fitness(ind, users_data, history) for ind in population]\n",
    "            print(f\"Best fitness in generation {generation+1}: {max(fitness_scores)}\")\n",
    "            \n",
    "            selected = []\n",
    "            for _ in range(self.pop_size):\n",
    "                candidates = random.sample(list(zip(population, fitness_scores)), 3)\n",
    "                best = max(candidates, key=lambda x: x[1])[0]\n",
    "                selected.append(best)\n",
    "            \n",
    "            \n",
    "            new_population = []\n",
    "            for i in range(0, self.pop_size, 2):\n",
    "                parent1, parent2 = selected[i], selected[i+1]\n",
    "                child1 = self.crossover(parent1, parent2, users_data)\n",
    "                child2 = self.crossover(parent2, parent1, users_data)\n",
    "                new_population.extend([\n",
    "                    self.mutate(child1, users_data),\n",
    "                    self.mutate(child2, users_data)\n",
    "                ])\n",
    "            \n",
    "            population = new_population\n",
    "        \n",
    "        best_individual = max(population, key=lambda ind: self.fitness(ind, users_data, history))\n",
    "        \n",
    "        result = []\n",
    "        for i, j in best_individual:\n",
    "            user1 = users_data[i][\"user_id\"]\n",
    "            user2 = users_data[j][\"user_id\"] if j != -1 else None\n",
    "            result.append((user1, user2))\n",
    "\n",
    "        return result\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7c2a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/100\n",
      "Best fitness in generation 1: 0.32473519444465637\n",
      "Generation 2/100\n",
      "Best fitness in generation 2: 0.34599342942237854\n",
      "Generation 3/100\n",
      "Best fitness in generation 3: 0.34599342942237854\n",
      "Generation 4/100\n",
      "Best fitness in generation 4: 0.34599342942237854\n",
      "Generation 5/100\n",
      "Best fitness in generation 5: 0.34599342942237854\n",
      "Generation 6/100\n",
      "Best fitness in generation 6: 0.3543155789375305\n",
      "Generation 7/100\n",
      "Best fitness in generation 7: 0.35586783289909363\n",
      "Generation 8/100\n",
      "Best fitness in generation 8: 0.35453101992607117\n",
      "Generation 9/100\n",
      "Best fitness in generation 9: 0.3737639784812927\n",
      "Generation 10/100\n",
      "Best fitness in generation 10: 0.376390278339386\n",
      "Generation 11/100\n",
      "Best fitness in generation 11: 0.3955304026603699\n",
      "Generation 12/100\n",
      "Best fitness in generation 12: 0.3955304026603699\n",
      "Generation 13/100\n",
      "Best fitness in generation 13: 0.3955304026603699\n",
      "Generation 14/100\n",
      "Best fitness in generation 14: 0.3955304026603699\n",
      "Generation 15/100\n",
      "Best fitness in generation 15: 0.3955304026603699\n",
      "Generation 16/100\n",
      "Best fitness in generation 16: 0.4403986930847168\n",
      "Generation 17/100\n",
      "Best fitness in generation 17: 0.4403986930847168\n",
      "Generation 18/100\n",
      "Best fitness in generation 18: 0.4403986930847168\n",
      "Generation 19/100\n",
      "Best fitness in generation 19: 0.4347616136074066\n",
      "Generation 20/100\n",
      "Best fitness in generation 20: 0.4347616136074066\n",
      "Generation 21/100\n",
      "Best fitness in generation 21: 0.4347616136074066\n",
      "Generation 22/100\n",
      "Best fitness in generation 22: 0.4347616136074066\n",
      "Generation 23/100\n",
      "Best fitness in generation 23: 0.4347616136074066\n",
      "Generation 24/100\n",
      "Best fitness in generation 24: 0.4347616136074066\n",
      "Generation 25/100\n",
      "Best fitness in generation 25: 0.4770452678203583\n",
      "Generation 26/100\n",
      "Best fitness in generation 26: 0.4801200330257416\n",
      "Generation 27/100\n",
      "Best fitness in generation 27: 0.4801200330257416\n",
      "Generation 28/100\n",
      "Best fitness in generation 28: 0.4557150900363922\n",
      "Generation 29/100\n",
      "Best fitness in generation 29: 0.4801200330257416\n",
      "Generation 30/100\n",
      "Best fitness in generation 30: 0.4801200330257416\n",
      "Generation 31/100\n",
      "Best fitness in generation 31: 0.48728397488594055\n",
      "Generation 32/100\n",
      "Best fitness in generation 32: 0.5087368488311768\n",
      "Generation 33/100\n",
      "Best fitness in generation 33: 0.5087368488311768\n",
      "Generation 34/100\n",
      "Best fitness in generation 34: 0.5087368488311768\n",
      "Generation 35/100\n",
      "Best fitness in generation 35: 0.5351260900497437\n",
      "Generation 36/100\n",
      "Best fitness in generation 36: 0.5351260900497437\n",
      "Generation 37/100\n",
      "Best fitness in generation 37: 0.5351260900497437\n",
      "Generation 38/100\n",
      "Best fitness in generation 38: 0.5351260900497437\n",
      "Generation 39/100\n",
      "Best fitness in generation 39: 0.5351260900497437\n",
      "Generation 40/100\n",
      "Best fitness in generation 40: 0.5351260900497437\n",
      "Generation 41/100\n",
      "Best fitness in generation 41: 0.5351260900497437\n",
      "Generation 42/100\n",
      "Best fitness in generation 42: 0.5351260900497437\n",
      "Generation 43/100\n",
      "Best fitness in generation 43: 0.5351260900497437\n",
      "Generation 44/100\n",
      "Best fitness in generation 44: 0.5351260900497437\n",
      "Generation 45/100\n",
      "Best fitness in generation 45: 0.5351260900497437\n",
      "Generation 46/100\n",
      "Best fitness in generation 46: 0.5351260900497437\n",
      "Generation 47/100\n",
      "Best fitness in generation 47: 0.5500401854515076\n",
      "Generation 48/100\n",
      "Best fitness in generation 48: 0.5500401854515076\n",
      "Generation 49/100\n",
      "Best fitness in generation 49: 0.5500401854515076\n",
      "Generation 50/100\n",
      "Best fitness in generation 50: 0.5576508641242981\n",
      "Generation 51/100\n",
      "Best fitness in generation 51: 0.5576508641242981\n",
      "Generation 52/100\n",
      "Best fitness in generation 52: 0.5576508641242981\n",
      "Generation 53/100\n",
      "Best fitness in generation 53: 0.5576508641242981\n",
      "Generation 54/100\n",
      "Best fitness in generation 54: 0.5576508641242981\n",
      "Generation 55/100\n",
      "Best fitness in generation 55: 0.5576508641242981\n",
      "Generation 56/100\n",
      "Best fitness in generation 56: 0.5576508641242981\n",
      "Generation 57/100\n",
      "Best fitness in generation 57: 0.5576508641242981\n",
      "Generation 58/100\n",
      "Best fitness in generation 58: 0.5576508641242981\n",
      "Generation 59/100\n",
      "Best fitness in generation 59: 0.5576508641242981\n",
      "Generation 60/100\n",
      "Best fitness in generation 60: 0.5576508641242981\n",
      "Generation 61/100\n",
      "Best fitness in generation 61: 0.5576508641242981\n",
      "Generation 62/100\n",
      "Best fitness in generation 62: 0.5576508641242981\n",
      "Generation 63/100\n",
      "Best fitness in generation 63: 0.5576508641242981\n",
      "Generation 64/100\n",
      "Best fitness in generation 64: 0.5576508641242981\n",
      "Generation 65/100\n",
      "Best fitness in generation 65: 0.5576508641242981\n",
      "Generation 66/100\n",
      "Best fitness in generation 66: 0.5576509237289429\n",
      "Generation 67/100\n",
      "Best fitness in generation 67: 0.5576509237289429\n",
      "Generation 68/100\n",
      "Best fitness in generation 68: 0.5576509237289429\n",
      "Generation 69/100\n",
      "Best fitness in generation 69: 0.5576509237289429\n",
      "Generation 70/100\n",
      "Best fitness in generation 70: 0.5576509237289429\n",
      "Generation 71/100\n",
      "Best fitness in generation 71: 0.5582507848739624\n",
      "Generation 72/100\n",
      "Best fitness in generation 72: 0.5405867695808411\n",
      "Generation 73/100\n",
      "Best fitness in generation 73: 0.5405867695808411\n",
      "Generation 74/100\n",
      "Best fitness in generation 74: 0.5405867695808411\n",
      "Generation 75/100\n",
      "Best fitness in generation 75: 0.5405867695808411\n",
      "Generation 76/100\n",
      "Best fitness in generation 76: 0.5405867695808411\n",
      "Generation 77/100\n",
      "Best fitness in generation 77: 0.5405867695808411\n",
      "Generation 78/100\n",
      "Best fitness in generation 78: 0.5405867695808411\n",
      "Generation 79/100\n",
      "Best fitness in generation 79: 0.5405867695808411\n",
      "Generation 80/100\n",
      "Best fitness in generation 80: 0.5405867695808411\n",
      "Generation 81/100\n",
      "Best fitness in generation 81: 0.5405867695808411\n",
      "Generation 82/100\n",
      "Best fitness in generation 82: 0.5426474213600159\n",
      "Generation 83/100\n",
      "Best fitness in generation 83: 0.5426474213600159\n",
      "Generation 84/100\n",
      "Best fitness in generation 84: 0.5426474213600159\n",
      "Generation 85/100\n",
      "Best fitness in generation 85: 0.5426473617553711\n",
      "Generation 86/100\n",
      "Best fitness in generation 86: 0.5426473617553711\n",
      "Generation 87/100\n",
      "Best fitness in generation 87: 0.5426473617553711\n",
      "Generation 88/100\n",
      "Best fitness in generation 88: 0.5610839128494263\n",
      "Generation 89/100\n",
      "Best fitness in generation 89: 0.5610839128494263\n",
      "Generation 90/100\n",
      "Best fitness in generation 90: 0.5605308413505554\n",
      "Generation 91/100\n",
      "Best fitness in generation 91: 0.5526365041732788\n",
      "Generation 92/100\n",
      "Best fitness in generation 92: 0.5526365041732788\n",
      "Generation 93/100\n",
      "Best fitness in generation 93: 0.5532363653182983\n",
      "Generation 94/100\n",
      "Best fitness in generation 94: 0.5526365041732788\n",
      "Generation 95/100\n",
      "Best fitness in generation 95: 0.5526365041732788\n",
      "Generation 96/100\n",
      "Best fitness in generation 96: 0.5526365041732788\n",
      "Generation 97/100\n",
      "Best fitness in generation 97: 0.5526365041732788\n",
      "Generation 98/100\n",
      "Best fitness in generation 98: 0.5526365041732788\n",
      "Generation 99/100\n",
      "Best fitness in generation 99: 0.5234999656677246\n",
      "Generation 100/100\n",
      "Best fitness in generation 100: 0.5341999530792236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', '18'),\n",
       " ('12', '13'),\n",
       " ('14', '19'),\n",
       " ('16', None),\n",
       " ('11', '31'),\n",
       " ('23', '29'),\n",
       " ('21', '20'),\n",
       " ('3', '25'),\n",
       " ('15', '30'),\n",
       " ('7', '4'),\n",
       " ('9', '6'),\n",
       " ('10', '26'),\n",
       " ('28', '22'),\n",
       " ('17', '2'),\n",
       " ('27', '8'),\n",
       " ('5', '24')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = GeneticMatcher(\n",
    "    tag_distance_func=\"semantic\",\n",
    "    coef_a=0.6,\n",
    "    coef_b=0.4,\n",
    "    penalty_multiplier=0.9,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    pop_size=6,\n",
    "    generations=100,\n",
    "    mutation_rate=0.4\n",
    ")\n",
    "\n",
    "matcher.match(users_data, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a5d93",
   "metadata": {},
   "source": [
    "## Greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f863c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        coef_a: float = 0.6,\n",
    "        coef_b: float = 0.4,\n",
    "        penalty_multiplier: float = 0.85,\n",
    "        tag_distance_func: str = \"jaccard\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    ):\n",
    "        self.coef_a = coef_a\n",
    "        self.coef_b = coef_b\n",
    "        self.penalty = penalty_multiplier\n",
    "        self.tag_distance = {\n",
    "            \"jaccard\": jaccard_distance,\n",
    "            \"semantic\": lambda t1, t2: semantic_tag_distance(t1, t2, self.embedder)\n",
    "        }.get(tag_distance_func, jaccard_distance)\n",
    "\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "\n",
    "    def get_weeks_since_last_meeting(self, user_id1: str, user_id2: str, history: List[Dict]) -> float:\n",
    "        for event in history:\n",
    "            if {event[\"user_id1\"], event[\"user_id2\"]} == {user_id1, user_id2}:\n",
    "                last_meeting = datetime.strptime(event[\"timestamp\"], \"%Y-%m-%d\")\n",
    "                return (datetime.now() - last_meeting).days / 7\n",
    "        return float(\"inf\") \n",
    "\n",
    "    def compute_pair_score(self, user1: Dict, user2: Dict, history: List[Dict]) -> float:\n",
    "        bio_sim = cosine_similarity([user1[\"bio_embedding\"]], [user2[\"bio_embedding\"]])[0][0]\n",
    "        \n",
    "        tag_dist = self.tag_distance(user1[\"tags\"], user2[\"tags\"])\n",
    "        \n",
    "        base_score = self.coef_a * bio_sim + self.coef_b * (1 - tag_dist)\n",
    "        \n",
    "        weeks_passed = self.get_weeks_since_last_meeting(user1[\"user_id\"], user2[\"user_id\"],history)\n",
    "        penalty = self.penalty ** weeks_passed if weeks_passed != float(\"inf\") else 0\n",
    "\n",
    "        return base_score * (1 - penalty)\n",
    "\n",
    "    def match(self, users_data: List[Dict], history: List[Dict] = None) -> List[Tuple[str, Optional[str]]]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "        \n",
    "        for user in users_data:\n",
    "            embedding = self.embedder.encode(user[\"bio\"])\n",
    "            if hasattr(embedding, \"cpu\"):\n",
    "                embedding = embedding.cpu().numpy()\n",
    "            user[\"bio_embedding\"] = embedding\n",
    "        \n",
    "        # list of all pairs of users with their scores\n",
    "        pairs = []\n",
    "        n = len(users_data)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                user1 = users_data[i]\n",
    "                user2 = users_data[j]\n",
    "                score = self.compute_pair_score(user1, user2, history)\n",
    "                pairs.append((score, i, j))\n",
    "        \n",
    "        pairs.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # greedy matching\n",
    "        matched_pairs = []\n",
    "        total_score = 0.0\n",
    "        pair_count = 0\n",
    "        used_indices = set()\n",
    "        \n",
    "        for score, i, j in pairs:\n",
    "            if i not in used_indices and j not in used_indices:\n",
    "                matched_pairs.append((users_data[i][\"user_id\"], users_data[j][\"user_id\"]))\n",
    "                total_score += score\n",
    "                pair_count += 1\n",
    "                used_indices.update([i, j])\n",
    "                if len(used_indices) == n:\n",
    "                    break\n",
    "        \n",
    "        all_indices = set(range(n))\n",
    "        leftover_indices = all_indices - used_indices\n",
    "        for idx in leftover_indices:\n",
    "            matched_pairs.append((users_data[idx][\"user_id\"], None)) # \"Sori brat\" case\n",
    "        \n",
    "        print(f\"Average score: {total_score / pair_count if pair_count > 0 else 0.0}\")\n",
    "        return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b15cd86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 0.6923855543136597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('12', '13'),\n",
       " ('2', '26'),\n",
       " ('7', '11'),\n",
       " ('20', '24'),\n",
       " ('1', '14'),\n",
       " ('4', '23'),\n",
       " ('15', '22'),\n",
       " ('19', '25'),\n",
       " ('17', '18'),\n",
       " ('6', '27'),\n",
       " ('5', '29'),\n",
       " ('8', '9'),\n",
       " ('21', '31'),\n",
       " ('3', '10'),\n",
       " ('16', '28'),\n",
       " ('30', None)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = GreedyMatcher(\n",
    "    coef_a=0.7, \n",
    "    coef_b=0.3,\n",
    "    penalty_multiplier=0.9,\n",
    "    tag_distance_func=\"jaccard\"\n",
    ")\n",
    "matcher.match(users_data, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c81c0",
   "metadata": {},
   "source": [
    "## Clustering + greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88215cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedClusterMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_clusters: int = 5,\n",
    "        coef_a: float = 0.6,\n",
    "        coef_b: float = 0.4,\n",
    "        penalty_multiplier: float = 0.85,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        tag_distance_func: str = \"jaccard\"\n",
    "    ):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.coef_a = coef_a\n",
    "        self.coef_b = coef_b\n",
    "        self.penalty = penalty_multiplier\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.tag_distance = {\n",
    "            \"jaccard\": jaccard_distance,\n",
    "            \"semantic\": lambda t1, t2: semantic_tag_distance(t1, t2, self.embedder)\n",
    "        }.get(tag_distance_func, jaccard_distance)\n",
    "\n",
    "    def match(self, users_data: List[Dict], history: Optional[List[Dict]] = None) -> Tuple[List[Tuple[str, Optional[str], float]], float]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "\n",
    "        embeddings: List = []\n",
    "        for user in users_data:\n",
    "            embedding = self.embedder.encode(user[\"bio\"])\n",
    "            if hasattr(embedding, \"cpu\"):\n",
    "                embedding = embedding.cpu().numpy()\n",
    "            user[\"bio_embedding\"] = embedding\n",
    "            embeddings.append(embedding)\n",
    "        kmeans = KMeans(n_clusters=min(self.n_clusters, len(users_data)//2), init='k-means++', random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "        cluster_users: DefaultDict[int, List[Dict]] = defaultdict(list)\n",
    "        for idx, cluster_id in enumerate(clusters):\n",
    "            cluster_users[cluster_id].append(users_data[idx])\n",
    "\n",
    "        main_pairs: List[Tuple[Dict, Dict, float]] = []\n",
    "        leftover_users: List[Dict] = []\n",
    "\n",
    "        for cluster_id, users in cluster_users.items():\n",
    "            if len(users) % 2 == 0:\n",
    "                cluster_pairs = self.greedy_match_in_cluster(users, history)\n",
    "                main_pairs.extend(cluster_pairs)\n",
    "            else:\n",
    "                cluster_pairs = self.greedy_match_in_cluster(users[:-1], history)\n",
    "                main_pairs.extend(cluster_pairs)\n",
    "                leftover_users.append(users[-1])\n",
    "\n",
    "        if len(leftover_users) >= 2:\n",
    "            leftover_pairs = self.greedy_match_in_cluster(leftover_users, history)\n",
    "            main_pairs.extend(leftover_pairs)\n",
    "            leftover_users = []\n",
    "\n",
    "        final_pairs: List[Tuple[str, Optional[str], float]] = []\n",
    "        total_score = 0.0\n",
    "        pair_count = 0\n",
    "\n",
    "        for pair in main_pairs:\n",
    "            final_pairs.append((pair[0][\"user_id\"], pair[1][\"user_id\"], pair[2]))\n",
    "            total_score += pair[2]\n",
    "            pair_count += 1\n",
    "\n",
    "        for user in leftover_users:\n",
    "            final_pairs.append((user[\"user_id\"], None, 0.0))\n",
    "\n",
    "        avg_score = total_score / pair_count if pair_count > 0 else 0.0\n",
    "\n",
    "        return final_pairs, avg_score\n",
    "\n",
    "    def greedy_match_in_cluster(self, users: List[Dict], history: List[Dict]) -> List[Tuple[Dict, Dict, float]]:\n",
    "        n = len(users)\n",
    "        if n < 2:\n",
    "            return []\n",
    "        pairs = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                score = self.compute_pair_score(users[i], users[j], history)\n",
    "                pairs.append((score, i, j))\n",
    "\n",
    "        pairs.sort(reverse=True, key=lambda x: x[0])\n",
    "        used = set()\n",
    "        result = []\n",
    "        for score, i, j in pairs:\n",
    "            if i not in used and j not in used:\n",
    "                result.append((users[i], users[j], score))\n",
    "                used.add(i)\n",
    "                used.add(j)\n",
    "        return result\n",
    "\n",
    "    def compute_pair_score(self, user1: Dict, user2: Dict, history: List[Dict]) -> float:\n",
    "        bio_sim = cosine_similarity([user1[\"bio_embedding\"]], [user2[\"bio_embedding\"]])[0][0]\n",
    "        tag_dist = self.tag_distance(user1[\"tags\"], user2[\"tags\"])\n",
    "        base_score = self.coef_a * bio_sim + self.coef_b * (1 - tag_dist)\n",
    "        weeks_passed = self.get_weeks_since_last_meeting(user1[\"user_id\"], user2[\"user_id\"], history)\n",
    "        penalty = self.penalty ** weeks_passed if weeks_passed != float(\"inf\") else 0\n",
    "        return base_score * (1 - penalty)\n",
    "\n",
    "    def get_weeks_since_last_meeting(self, user_id1: str, user_id2: str, history: List[Dict]) -> float:\n",
    "        for event in history:\n",
    "            if {event[\"user_id1\"], event[\"user_id2\"]} == {user_id1, user_id2}:\n",
    "                last_meeting = datetime.strptime(event[\"timestamp\"], \"%Y-%m-%d\")\n",
    "                return (datetime.now() - last_meeting).days / 7\n",
    "        return float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb35f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 0.6501028537750244\n",
      "(20, 24)\n",
      "(1, 14)\n",
      "(5, 29)\n",
      "(21, 31)\n",
      "(2, 26)\n",
      "(19, 25)\n",
      "(17, 18)\n",
      "(3, 10)\n",
      "(7, 11)\n",
      "(15, 22)\n",
      "(4, 16)\n",
      "(6, 27)\n",
      "(8, 9)\n",
      "(12, 13)\n",
      "(28, 30)\n",
      "(23, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "matcher = BalancedClusterMatcher(\n",
    "    n_clusters=5,\n",
    "    coef_a=0.7, \n",
    "    coef_b=0.3,\n",
    "    penalty_multiplier=0.9\n",
    ")\n",
    "result, avg_score = matcher.match(users_data, history)\n",
    "print(f\"Average score: {avg_score}\")\n",
    "for user1, user2, score in result:\n",
    "    print(f\"({user1}, {user2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a9398",
   "metadata": {},
   "source": [
    "## Clustering + Simulated Anhealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc6f450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedClusterAnnealingMatcher:\n",
    "    def __init__(self,\n",
    "                 n_clusters: int = 5,\n",
    "                 coef_a: float = 0.6,\n",
    "                 coef_b: float = 0.4,\n",
    "                 initial_temp: float = 1000,\n",
    "                 cooling_rate: float = 0.95,\n",
    "                 n_iterations: int = 500,\n",
    "                 penalty_multiplier: float = 0.85,\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 tag_distance_func: str = \"jaccard\"):\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.coef_a = coef_a\n",
    "        self.coef_b = coef_b\n",
    "        self.initial_temp = initial_temp\n",
    "        self.cooling_rate = cooling_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.penalty = penalty_multiplier\n",
    "        self.embedder = SentenceTransformer(embedding_model)\n",
    "        self.tag_distance = {\n",
    "            \"jaccard\": jaccard_distance,\n",
    "            \"semantic\": lambda t1, t2: semantic_tag_distance(t1, t2, self.embedder)\n",
    "        }.get(tag_distance_func, jaccard_distance)\n",
    "\n",
    "    def cluster_users(self, users_data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        bio_embeddings = np.array([user[\"bio_embedding\"] for user in users_data])\n",
    "        \n",
    "        tag_embeddings = []\n",
    "        for user in users_data:\n",
    "            if hasattr(self.tag_distance, '__name__') and self.tag_distance.__name__ == '_semantic_tag_distance':\n",
    "                tags_text = \" \".join(user[\"tags\"])\n",
    "                tag_emb = self.embedder.encode(tags_text, convert_to_tensor=False)\n",
    "                tag_embeddings.append(tag_emb)\n",
    "            else:\n",
    "                # for jaccard\n",
    "                all_tags = list(set(tag for user in users_data for tag in user[\"tags\"]))\n",
    "                tag_vec = np.zeros(len(all_tags))\n",
    "                for tag in user[\"tags\"]:\n",
    "                    if tag in all_tags:\n",
    "                        tag_vec[all_tags.index(tag)] = 1\n",
    "                tag_embeddings.append(tag_vec)\n",
    "        \n",
    "        tag_embeddings = np.array(tag_embeddings)\n",
    "        \n",
    "        bio_embeddings = bio_embeddings / np.linalg.norm(bio_embeddings, axis=1, keepdims=True)\n",
    "        tag_embeddings = tag_embeddings / np.linalg.norm(tag_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        combined_embeddings = np.hstack([\n",
    "            self.coef_a * bio_embeddings,\n",
    "            self.coef_b * tag_embeddings\n",
    "        ])\n",
    "        \n",
    "        kmeans = KMeans(\n",
    "            n_clusters=min(self.n_clusters, len(users_data)//2),\n",
    "            init='k-means++',\n",
    "            random_state=42\n",
    "        )\n",
    "        labels = kmeans.fit_predict(combined_embeddings)\n",
    "        return labels, kmeans.cluster_centers_\n",
    "\n",
    "    def compute_pair_score(self, user1: Dict, user2: Dict, history: List[Dict]) -> float:\n",
    "        bio_sim = cosine_similarity([user1[\"bio_embedding\"]], [user2[\"bio_embedding\"]])[0][0]\n",
    "        \n",
    "        tag_dist = self.tag_distance(user1[\"tags\"], user2[\"tags\"])\n",
    "        \n",
    "        base_score = self.coef_a * bio_sim + self.coef_b * (1 - tag_dist)\n",
    "\n",
    "        weeks_passed = self.get_weeks_since_last_meeting(user1[\"user_id\"], user2[\"user_id\"], history)\n",
    "        penalty = self.penalty ** weeks_passed if weeks_passed != float(\"inf\") else 0\n",
    "\n",
    "        return base_score * (1 - penalty)\n",
    "\n",
    "    def get_weeks_since_last_meeting(self, user_id1: str, user_id2: str, history: List[Dict]) -> float:\n",
    "        for event in history:\n",
    "            if {event[\"user_id1\"], event[\"user_id2\"]} == {user_id1, user_id2}:\n",
    "                last_meeting = datetime.strptime(event[\"timestamp\"], \"%Y-%m-%d\")\n",
    "                return (datetime.now() - last_meeting).days / 7\n",
    "        return float(\"inf\")\n",
    "\n",
    "    def initial_solution(self, clustered_users: DefaultDict[int, List[Dict]]) -> List[List[Optional[Dict]]]:\n",
    "        solution = []\n",
    "        leftover_users = []\n",
    "        \n",
    "        for cluster_id, users in clustered_users.items():\n",
    "            random.shuffle(users)\n",
    "            for i in range(0, len(users)-1, 2):\n",
    "                solution.append([users[i], users[i+1]])\n",
    "            if len(users) % 2 != 0:\n",
    "                leftover_users.append(users[-1])\n",
    "        \n",
    "        random.shuffle(leftover_users)\n",
    "        for i in range(0, len(leftover_users)-1, 2):\n",
    "            solution.append([leftover_users[i], leftover_users[i+1]])\n",
    "        if len(leftover_users) % 2 != 0:\n",
    "            solution.append([leftover_users[-1], None])\n",
    "            \n",
    "        return solution\n",
    "\n",
    "    def energy(self, solution: List[List[Optional[Dict]]], history: List[Dict]) -> float:\n",
    "        scores = [self.compute_pair_score(p[0], p[1], history) for p in solution if p[1] is not None]\n",
    "        return -np.mean(scores) if scores else float('inf')\n",
    "\n",
    "    def get_neighbor(self, solution: List[List[Optional[Dict]]]) -> List[List[Optional[Dict]]]:\n",
    "        neighbor = deepcopy(solution)\n",
    "        \n",
    "        proper_pairs = [p for p in neighbor if p[1] is not None]\n",
    "        if len(proper_pairs) < 2:\n",
    "            return neighbor\n",
    "            \n",
    "        if random.random() < 0.7:  # intra-cluster swap\n",
    "            cluster_pairs = [p for p in proper_pairs if p[0][\"cluster_id\"] == p[1][\"cluster_id\"]]\n",
    "            if len(cluster_pairs) >= 2:\n",
    "                i, j = random.sample(range(len(cluster_pairs)), 2)\n",
    "                swap_idx = random.randint(0, 1)\n",
    "                cluster_pairs[i][swap_idx], cluster_pairs[j][swap_idx] = cluster_pairs[j][swap_idx], cluster_pairs[i][swap_idx]\n",
    "        else:  # inter-cluster move\n",
    "            i, j = random.sample(range(len(proper_pairs)), 2)\n",
    "            swap_idx = random.randint(0, 1)\n",
    "            proper_pairs[i][swap_idx], proper_pairs[j][swap_idx] = proper_pairs[j][swap_idx], proper_pairs[i][swap_idx]\n",
    "        \n",
    "        return neighbor\n",
    "\n",
    "    def match(self, users_data: List[Dict], history: Optional[List[Dict]] = None) -> Tuple[List[Tuple[str, Optional[str], float]], float, float]:\n",
    "        if history is None:\n",
    "            history = []\n",
    "            \n",
    "        for user in users_data:\n",
    "            user[\"bio_embedding\"] = self.embedder.encode(user[\"bio\"])\n",
    "            if hasattr(user[\"bio_embedding\"], \"cpu\"):\n",
    "                user[\"bio_embedding\"] = user[\"bio_embedding\"].cpu().numpy()\n",
    "\n",
    "        cluster_labels, cluster_centers = self.cluster_users(users_data)\n",
    "        for i, user in enumerate(users_data):\n",
    "            user[\"cluster_id\"] = int(cluster_labels[i])\n",
    "            user[\"cluster_embedding\"] = cluster_centers[cluster_labels[i]]\n",
    "        \n",
    "        clustered_users = defaultdict(list)\n",
    "        for user in users_data:\n",
    "            clustered_users[user[\"cluster_id\"]].append(user)\n",
    "        \n",
    "        current_solution = self.initial_solution(clustered_users)\n",
    "        current_energy = self.energy(current_solution, history)\n",
    "        best_solution = deepcopy(current_solution)\n",
    "        best_energy = current_energy\n",
    "        temp = self.initial_temp\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            neighbor = self.get_neighbor(current_solution)\n",
    "            neighbor_energy = self.energy(neighbor, history)\n",
    "\n",
    "            if neighbor_energy < current_energy or random.random() < exp((current_energy - neighbor_energy)/temp):\n",
    "                current_solution = neighbor\n",
    "                current_energy = neighbor_energy\n",
    "                if neighbor_energy < best_energy:\n",
    "                    best_solution = deepcopy(neighbor)\n",
    "                    best_energy = neighbor_energy\n",
    "            \n",
    "            temp *= self.cooling_rate\n",
    "        \n",
    "        result = []\n",
    "        valid_scores = []\n",
    "        cluster_matches = 0\n",
    "        \n",
    "        for pair in best_solution:\n",
    "            user1, user2 = pair[0], pair[1]\n",
    "            score = self.compute_pair_score(user1, user2, history) if user2 else 0.0\n",
    "            result.append((user1[\"user_id\"], user2[\"user_id\"] if user2 else None, score))\n",
    "            \n",
    "            if user2:\n",
    "                valid_scores.append(score)\n",
    "                if user1[\"cluster_id\"] == user2[\"cluster_id\"]:\n",
    "                    cluster_matches += 1\n",
    "        \n",
    "        avg_score = np.mean(valid_scores) if valid_scores else 0.0\n",
    "        cluster_homogeneity = cluster_matches / len(valid_scores) if valid_scores else 0.0\n",
    "        \n",
    "        return result, avg_score, cluster_homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f415013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/rail/Library/Python/3.9/lib/python/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 0.6503289341926575\n",
      "(1, 14)\n",
      "(10, 26)\n",
      "(3, 2)\n",
      "(18, 25)\n",
      "(17, 19)\n",
      "(23, 11)\n",
      "(20, 29)\n",
      "(24, 21)\n",
      "(7, 4)\n",
      "(31, 5)\n",
      "(9, 8)\n",
      "(6, 27)\n",
      "(15, 28)\n",
      "(30, 12)\n",
      "(13, 22)\n",
      "(16, None)\n"
     ]
    }
   ],
   "source": [
    "matcher = OptimizedClusterAnnealingMatcher(\n",
    "    n_clusters=5,\n",
    "    coef_a=0.7,\n",
    "    coef_b=0.3,\n",
    "    initial_temp=1000,\n",
    "    cooling_rate=0.9,\n",
    "    n_iterations=100,\n",
    "    penalty_multiplier=0.9,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    tag_distance_func=\"semantic\"\n",
    ")\n",
    "\n",
    "result, avg_score, cluster_homogeneity = matcher.match(users_data, history)\n",
    "print(f\"Average score: {avg_score}\")\n",
    "for user1, user2, score in result:\n",
    "    print(f\"({user1}, {user2})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
